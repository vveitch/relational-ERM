{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Relational ERM in the Skipgram model\n",
    "\n",
    "The skipgram model is a commonly used model in language and graph representation learning. In this notebook, we demonstrate how to simply implement the skipgram model in the relational ERM framework."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# book-keeping\n",
    "import sys\n",
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "\n",
    "sys.path.append('../')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Model\n",
    "\n",
    "We first create the model for the skipgram. The skipgram model attempts to predict the presence or absence of an edge between two vertices from the vertex embeddings. Here, we create a tensorflow model which models this description for a given minibatch. For simplicity, we are using the tensorflow [estimator](https://www.tensorflow.org/programmers_guide/estimators) API."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def model_fn(features, labels, mode, params):\n",
    "    \"\"\" This function creates the skipgram model for semi-supervised node classification.\n",
    "    \n",
    "    It creates the necessary variables, computes the loss, and creates a training operation\n",
    "    to run to optimize the model.\n",
    "    \n",
    "    \"\"\"\n",
    "    # This variable corresponds to the vertex level embeddings.\n",
    "    embedding_variables = tf.get_variable(\n",
    "        'input_layer/vertex_index_embedding/embedding_weights',\n",
    "        shape=(params['num_vertices'], params['embedding_dimension']),\n",
    "        dtype=tf.float32,\n",
    "        initializer=tf.truncated_normal_initializer(stddev=1 / params['embedding_dimension']),\n",
    "        trainable=True)\n",
    "    \n",
    "    # vertex_index is a vector which represents the indices of the vertices that are present\n",
    "    # in the subsample.\n",
    "    vertex_index = features['vertex_index']\n",
    "    \n",
    "    # We gather the embeddings for the vertices in the subgraph\n",
    "    embeddings = tf.nn.embedding_lookup(embedding_variables, vertex_index)\n",
    "    \n",
    "    # In the semi-supervised node classification problem, each vertex is also given a label.\n",
    "    # A portion of the labels are censored at training time, given by `split`\n",
    "    vertex_labels = labels['labels']\n",
    "    vertex_labels_split = labels['split']\n",
    "    \n",
    "    # Our strategy computes two different losses: a skipgram loss on edges, and a logistic\n",
    "    # regression loss on vertices. Let's start with the skipgram loss.\n",
    "    label_task_weight = 1e-3\n",
    "    \n",
    "    # -------------------- SKIPGRAM LOSS ------------------------\n",
    "    edge_list = features['edge_list']\n",
    "    \n",
    "    # We use weight to denote whether an edge is the edge list is an actual edge or\n",
    "    # a non-edge.\n",
    "    edge_weight = tf.squeeze(features['weights'], axis=-1)\n",
    "    \n",
    "    # in the skipgram model, the edge prediction is based on a bernoulli model\n",
    "    # conditional on the dot product of the embeddings of the vertices.\n",
    "    #\n",
    "    # For computational efficiency, we compute the product for pairs of vertices,\n",
    "    # then select the ones corresponding to the edges and non-edges in the subsample.\n",
    "    embeddings_prod = tf.matmul(embeddings, embeddings, transpose_b=True)\n",
    "    edge_logit = tf.gather_nd(embeddings_prod, edge_list)\n",
    "    \n",
    "    # The loss is given by the sigmoid cross entropy.\n",
    "    edge_loss_per_edge = tf.nn.sigmoid_cross_entropy_with_logits(labels=edge_weight, logits=edge_logit)\n",
    "    edge_loss = tf.reduce_sum(edge_loss_per_edge)\n",
    "    \n",
    "    edge_accuracy = tf.metrics.accuracy(labels=edge_weight, predictions=tf.to_float(tf.greater(edge_logit, 0.5)))\n",
    "    \n",
    "    # -------------------- VERTEX LOSS ----------------------------\n",
    "    # the other aspect we tackle is a logistic regression of the vertex label onto the corresponding\n",
    "    # embeddings.\n",
    "\n",
    "    vertex_logits = tf.layers.dense(\n",
    "        embeddings, params['num_labels'], activation=None, use_bias=True,\n",
    "        kernel_regularizer=tf.contrib.layers.l2_regularizer(scale=label_task_weight * params['l2_regularization']))\n",
    "    \n",
    "    vertex_loss_per_vertex = tf.losses.sigmoid_cross_entropy(\n",
    "        vertex_labels, logits=vertex_logits, weights=tf.expand_dims(vertex_labels_split, -1),\n",
    "        reduction=tf.losses.Reduction.NONE)\n",
    "    \n",
    "    vertex_loss = tf.reduce_sum(vertex_loss_per_vertex)\n",
    "    \n",
    "    # -------------------- Optimization -----------------------------\n",
    "    # Having computed the vertex and edge loss, we combine them in a weighted fashion and\n",
    "    # apply a simple stochastic optimizer.\n",
    "    \n",
    "    total_loss = (1 - label_task_weight) * edge_loss + label_task_weight * vertex_loss\n",
    "    optimizer = tf.train.GradientDescentOptimizer(learning_rate=params['learning_rate'])\n",
    "    \n",
    "    train_op = optimizer.minimize(\n",
    "        total_loss, global_step=tf.train.get_or_create_global_step())\n",
    "    \n",
    "    return tf.estimator.EstimatorSpec(\n",
    "        mode, loss=total_loss, train_op=train_op,\n",
    "        eval_metric_ops={\n",
    "            'edge_accuracy': edge_accuracy\n",
    "        })"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Sampler\n",
    "\n",
    "In the relational ERM framework, the loss function and predictor are only half the story. The way in which we sample is central to the problem and is a part of the model definition. We use custom samplers and adapters we have developed to create an efficient input pipeline to produce the samples. In this case, we are using the tensorflow dataset API."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from relational_sgd.sampling import adapters, negative_sampling\n",
    "from relational_sgd.tensorflow_ops import dataset_ops\n",
    "\n",
    "def make_input_fn(graph, labels):\n",
    "    def input_fn(params):\n",
    "        # We first create a dataset which produces uniform random walks,\n",
    "        # i.e. list of vertex indices of the given length.\n",
    "        dataset = dataset_ops.RandomWalkDataset(\n",
    "            params['walk_length'], graph.neighbours, graph.lengths, graph.offsets)\n",
    "        \n",
    "        # We will apply several adapters to transform this dataset into a dataset\n",
    "        # which contains all the required information for the model function\n",
    "        num_vertices = len(graph.lengths)\n",
    "        \n",
    "        dataset = dataset.map(\n",
    "            adapters.compose(\n",
    "                # We first transform our list of vertices into an edge list which\n",
    "                # corresponds to the windowed edges.\n",
    "                adapters.adapt_random_walk_window(params['window_size']),\n",
    "                # We then add negative edges according to the described negative sampling\n",
    "                # scheme.\n",
    "                negative_sampling.add_negative_sample(\n",
    "                    num_vertices,\n",
    "                    num_samples_per_vertex=params['num_negative_samples'],\n",
    "                    vertex_distribution_logit=negative_sampling.make_learned_unigram_logits(\n",
    "                        num_vertices, prior=graph.lengths)),\n",
    "                # The next couple of operations are mostly bookkeeping to augment the graph with the\n",
    "                # necessary metadata\n",
    "                adapters.relabel_subgraph(),\n",
    "                adapters.append_vertex_labels(labels),\n",
    "                adapters.split_vertex_labels(num_vertices, proportion_censored=0.5),\n",
    "                adapters.format_features_labels()),\n",
    "            num_parallel_calls=2)\n",
    "        \n",
    "        dataset = dataset.prefetch(tf.contrib.data.AUTOTUNE)\n",
    "        return dataset\n",
    "    return input_fn"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Training the Model\n",
    "\n",
    "Once we have defined the above two components, we can load the data and train the model using the usual tensorflow tools. Below we illustrate with an example using the protein-protein interaction network in Homo-Sapiens."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def load_data(path=None):\n",
    "    from relational_sgd.graph_ops.representations import create_packed_adjacency_list, edge_list_to_adj_list\n",
    "    \n",
    "    if path is None:\n",
    "        path = 'data/homo_sapiens.npz'\n",
    "\n",
    "    with tf.gfile.Open(path, mode='rb') as f:\n",
    "        loaded = np.load(f, allow_pickle=False)\n",
    "    \n",
    "    edge_list = loaded['edge_list'].astype(np.int32)\n",
    "    \n",
    "    weights = np.ones(edge_list.shape[0], dtype=np.float32)\n",
    "    labels = loaded['group'].astype(np.int32)\n",
    "    \n",
    "    not_self_edge = edge_list[:, 0] != edge_list[:, 1]\n",
    "    edge_list = edge_list[not_self_edge, :]\n",
    "    weights = weights[not_self_edge]\n",
    "    \n",
    "    adjacency_list = edge_list_to_adj_list(edge_list, weights)\n",
    "    adjacency_list = create_packed_adjacency_list(adjacency_list)\n",
    "    \n",
    "    return adjacency_list, labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "graph, labels = load_data()\n",
    "num_vertices = len(graph.lengths)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "params = {\n",
    "    'num_vertices': num_vertices,\n",
    "    'embedding_dimension': 128,\n",
    "    'num_labels': labels.shape[1],\n",
    "    'walk_length': 80,\n",
    "    'window_size': 10,\n",
    "    'num_negative_samples': 5,\n",
    "    'learning_rate': 0.025,\n",
    "    'l2_regularization': 1\n",
    "}\n",
    "\n",
    "estimator = tf.estimator.Estimator(\n",
    "    model_fn=model_fn,\n",
    "    params=params)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tf.logging.set_verbosity(tf.logging.INFO)\n",
    "estimator.train(make_input_fn(graph, labels), steps=10000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "estimator.evaluate(make_input_fn(graph, labels), steps=20)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
